
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <link rel="stylesheet" href="Assets/style.css">

        <!-- =====BOX ICONS===== -->
        <link href='https://cdn.jsdelivr.net/npm/boxicons@2.0.5/css/boxicons.min.css' rel='stylesheet'>

        <title> IMAGE CAPTION </title>
    </head>
    <body>
        <!--===== HEADER =====-->
        <header class="l-header">
            <nav class="nav bd-grid">
                <div>
                    <a href="#" class="nav__logo"><<<strong>IMAGE CAPTION</strong></a>
                </div>

                <div class="nav__menu" id="nav-menu">
                    <ul class="nav__list">
                        <li class="nav__item"><a href="#group" class="nav__link active">Group Member</a></li>
                        <li class="nav__item"><a href="#acknowledment" class="nav__link">Acknowledment</a></li>
                        <li class="nav__item"><a href="#abstract" class="nav__link">Abstract</a></li>
                        <li class="nav__item"><a href="#about_Project" class="nav__link">About Project</a></li>
                        <li class="nav__item"><a href="#contact" class="nav__link">Contact</a></li>
                    </ul>
                </div>

                <div class="nav__toggle" id="nav-toggle">
                    <i class='bx bx-menu'></i>
                </div>
            </nav>
        </header>

        <main class="l-main">
            <!--===== HOME =====-->
            <section class="home bd-grid" id="home">




                <div class="home__data">
                    <h1 class="home__title"> JAYPEE INSTITUTE INFORMATION TECHNOLOGY, <span class="home__title-color">NOIDA</span><br>(Minor Project-1) </h1>
                    <div class="home__img">
                        <img src="Assets/image/jiit.jpg" alt="">
                    </div>

                </div>


            </section>

            <!--===== ABOUT =====-->
            <section class="about section " id="about">
                <h2 class="section-title">Group Mmber</h2>

                <div class="about__container bd-grid">


                    <div>
                        <h2 class="about__subtitle">GROUP MEMBERS: </h2>
                        <p class="about__text">  <div class="group_member">

                            <table>
                              <thead><h2>

                                 <th><td>Name</td><td>-------</td></th>
                                <th><td>En Roll</td></th>

                              </h2>

                              </thead>


                              <tbody>
                                <h3>
                                   <th><td><strong>1. </strong> Mayank Sharma</td><td>-------</td></th>
                                  <th><td>9918103045</td></th>
                                </h3>
                                </tbody>
                                <tbody>
                                  <h3>
                                     <th><td><strong>2. </strong> Ujjwal Singh Negi</td><td>-------</td></th>
                                    <th><td>9918103056</td></th>
                                  </h3>
                                  </tbody>
                                  <tbody>
                                    <h3>
                                       <th><td><strong>3. </strong> Mohammad Amazad Khan</td><td>-------</td></th>
                                      <th><td>9918103245</td></th>
                                    </h3>
                                    </tbody>
                                    <tbody>
                                      <h3>
                                         <th><td><strong>4. </strong> Utkarsh Sharma</td><td>-------</td></th>
                                        <th><td>9918103057</td></th>
                                      </h3>
                                      </tbody>

                              </thead>
                            </table>
                          </div></p>
                    </div>
                </div>
            </section>

            <!--===== SKILLS =====-->
            <section class="acknowledment section" id="acknowledment">
                <h2 class="section-title">Acknowledment</h2>

                <div class="acknowledment__container bd-grid">
                    <div>
                        <h2 class="acknowledment__subtitle">Acknowledment</h2>
                        <p class="__text">We would like to place on record our deep sense of gratitude to <strong>Mr Shariq Murtuza,</strong>  Professor, Jaypee Institute of Information Technology, Noida for his generous guidance, encouragement and supervision throughout the course of present work. We would like to thank our institute for providing us with the opportunity and resources required for the completion of the project. We would also like to thank authors of various books, research papers and journals that we used as references to shape our project.</p>

                    </div>


                </div>
            </section>

            <!--===== WORK =====-->
            <section class="abstract section" id="abstract">
                <h2 class="section-title">Abstract</h2>

                <div class="abstract__container bd-grid">
                  Generating a natural language description from images is an important problem at
the section of computer vision, natural language processing, artificial intelligence
and image processing. Observing  many recent  works in  deep  learning sector, we
introduced  a  hybrid RNN  model  which  is  generating text  from  the  given  input
images. We presented the learning model that generates natural language of images.
The model  utilized the  connections between natural  language and  visual data  by
produced text line based contents from a given image. Our Hybrid Recurrent Neural
Network  model  is  based  on  the  combination  of  Convolutional  Neural  Network
(CNN),  Long Short-Term  Memory  (LSTM) and  Bi-directional  Recurrent Neural
Network (BRNN) models. We used three benchmark datasets: Flickr8K, Flickr30K
and  MS COCO  for  training  our model  and observed  the accuracy  improvement
comparing with the  state of  the art  work.   A new English  dataset  is also  created
which is made to generate English caption from given input image. This dataset contains 8,000 images and all the images are in English perspective images. Our hybrid model learns
from a new set of data and  annotations  that reflect the  geographical context.
                </div>
            </section>

<!--about project-->
<section class="about_project section" id="about_project">
    <h2 class="section-title">About Project</h2>

    <div class="about_projevct__container bd-grid">
    <h2>INTRODUCTION</h2>
    Automatically generating captions to an image shows the understanding of the image by computers, which is a fundamental task of intelligence. For a caption model it not only needs to find which objects are contained in the image and also need to be able to expressing their relationships in a natural language such as English. Recent work also achieves the presence of attention, which can store and report the information and relationship between some most salient features and clusters in the image.
    </div>

    <div class="about_projevct__container bd-grid">
      <h2>MOTIVATION</h2>
      One  primary motivation  of  computational  visual recognition  models  is to
emulate  remarkable  human  capability  to  comprehend  visual  scenes  and  extract
detailed information from them with astonishing accuracy .  Many sophisticated
models have been developed  to extract visual  information from  images based  on
visual categorization of objects in the images . The visual recognition procedures
thus  pursued  in  most  cases  are  demanding  both  in  terms  of  computational
complexity  and obtaining  desired  accuracy.  One popular  endeavor  of  the visual
recognition modeling is to extract concise natural language description of an image,
i.e., to generate a single sentence representing an image most faithfully by reducing
the complexities


In our project, we do image-to-sentence generation. This application bridges vision and natural language. If we can do well in this task, we can then utilize natural language processing technologies understand the world in images. In addition, we introduced attention mechanism, which is able to recognize what a word refers to in the image, and thus summarize the relationship between objects in the image. This will be a powerful tool to utilize the massive unformatted image data, which dominate the whole data in the world.

    </div>
    <div class="about_projevct__container bd-grid">
      <h2> COMPUTATIONAL MODEL</h2>
      Deep learning is a machine learning technique that teaches computers to do
  what  comes  naturally  to  humans:  learn  by  example.  Deep  learning  is  a  key
  technology  behind driverless  cars,  enabling  them  to  recognize  a stop  sign,  or  to
  distinguish a pedestrian from a lamppost. It is the key to voice control in consumer
  devices like phones, tablets, TVs, and hands-free speakers. Deep learning is getting
  lots  of  attention  lately  and  for  good  reason.  It’s  achieving  results  that  were  not
  possible before.
  In  deep learning,  a computer  model  learns  to perform  classification tasks
  directly from images, text, or sound. Deep learning models can achieve state-of-the-
  art accuracy, sometimes exceeding human-level performance. Models are trained by
  using a large set of labeled data and neural network architectures that contain many
  layers.

    </div>
    <div class="about_projevct__container bd-grid">
      <h2> CONVOLUTIONAL NEURAL NETWORK</h2>
      A  Convolutional  Neural  Network  (CNN)  is  contained  at  least  one
convolutional layers and after that took after by at least one completely associated
layer  as  in  a  standard  multilayer  neural  system.  The  architecture  of  a  CNN  is
intended to exploit the 2D structure of an information picture. This is accomplished
with nearby associations and tied weights took after by some type of pooling which
brings about interpretation invariant highlights. Another advantage of CNNs is that
they  are  less  demanding  to  prepare  and  have  numerous  less  parameters  than
completely  associated  systems with  a similar  number of  concealed units.  In  this
article  we  will talk  about  the  engineering  of  a  CNN  and  the back  engendering
calculation to process the inclination regarding the parameters of the model so as to
utilize  angle  based  enhancement.
 <br>
 <h3>CNN basically use for image recognition, video
analysis system, natural language processing and many more. In CNN, input layer,
convolutional layer, polling layer, fully connected layer and output layer exist .
In input layer there are three measurements and they are width, height and depth. It
is a framework of pixel esteem. At  that point  the  convolutional layer existing. A
piece of the picture is associated with the following Convolutional layer in light of
the  fact  that  if  every  one  of  the  pixels  of  the  info  is  associated  with  the
Convolutional layer. Filter, Kernel, or  Feature Detector is a little matrix used for
highlights location. After convolutional  layer,  at that  point the  pooling layer part
exists. Pool Layer plays out a capacity to decrease the spatial measurements of the
information, and the computational  unpredictability of our  model. What’s  more, it
additionally  controls over  fitting. After  pooling  layer,  fully connected  layer  part
existing and fully connected layers interface each neuron in one layer to each neuron
in another layer. The last fully connected layer utilizes a softmax initiation work for
characterizing  the  produced  highlights  of  the  information  picture  into  different
classes in light of the training dataset and after completing this layer then we get an
output
 </h3>
    </div>
    <div class="about_projevct__container bd-grid">
      <h2> RECURRENT NEURAL NETWORK</h2>
      Recurrent  neural  network  is  a  class  of  simulated  neural  system  where
  associations between units shape a coordinated diagram along an arrangement. This
  enables it to show dynamic transient conduct for a period  grouping. The thought
  behind RNNs is to make utilization of consecutive data. Recurrent Neural Network
  takes the previous output or hidden state as inputs. RNN are helpful for their middle
  of the road esteems can  store data about past contributions for a period that isn't
  settled  from  the earlier.  RNN  basically  utilized  for language  demonstrating  and
  creating  content,  machine  translation,  speech  recognition,  generating  image
  description. In RNN, process sequences are different type like as one to one, one to
  many, many to one, many to many existing [20].

  A  RNN  is  a  sort  of  neural  systems,  which  can  send  input  signals,  for
  example, Hopfield  net and long-here  and now memory  (LSTM). RNN models  a
  dynamic  framework,  where the  hidden  state h(t)   is not  just  reliant  on  the present
  perception x(t)  , yet additionally depends on the previous hidden state h(t-1) . We can
  represent h(t)  like this

                                     h(t)=f(h(t-1),x(t))
  Where  f  is  non-linear  mapping.  From  the  above  equation,  we  got  h(t)  which  is
  contains  information  about  the  whole  sequence,  which  can be  derived  from  the
  recursive  definition  in  above  equation.  As  such,  RNN  can  utilize  the  concealed
  factors as a memory to catch long haul data from a sequence. We are following RNN
  model, such that

    </div>
    <div class="about_projevct__container bd-grid">
      <h2>     LONG SHORT TERM MODEL </h2>

      Long short-term memory  (LSTM) units are a  building unit for layers of a
      repetitive neural system (RNN). The LSTM utilizes this thought of “Constant Error
      Flow”  for RNNs  to make  a  “Constant  Error  Carousel”  (CEC)  which  ensures  that
      gradients  don’t  decay. A  LSTM unit  is  fixed  of  a  cell,  an  input,  an output and  a
      forget  gate.  LSTMs  are  an  uncommon  sort  of  RNN,  fit  for  adapting  long  haul
      conditions.  LSTM  mainly  used  for  robot  controlling,  rhythm  learning,  speech
      recognition,  grammar  learning,  human  action  recognition,  sign  language
      recognition, semantic parsing [22].  LSTM cell stores an esteem which is  long or
      brief eras. This is accomplished by utilizing initiation  work for the  memory cell.
      LSTM were intended to battle vanishing slopes through a gating instrument.



      Long  short-term  memory  (LSTM) are  a  special  part  of  RNN  (Recurrent
      Neural  Network)  equipped  for  learning  long-term  dependencies.  LSTM  are
      unequivocally intended to maintain a strategic distance from the long haul reliance
      issue. Recalling data for significant lots of time is for all intents and purposes their
      default conduct, not  something they battle  to learn.  For discuss  about the  LSTM
      network, initial phase in our LSTM is to choose what data we will discard from the
      cell state.


    </div>
    <div class="about_projevct__container bd-grid">
      <h2>   DATA SET </h2>

      We  utilize  the  Flickr8K,  Flickr30K,  and  MSCOCO  datasets  for  our
      experiment. Flickr8K dataset contain 8,000,  Flickr30K dataset contain 31,000 and
      MSCOCO dataset contain 123,000 images. For Flickr8K and Flickr30K dataset, we
      utilize  1,000  pictures  for  validation,  1,000  for  testing  and  the  rest  pictures  for
      training. For MS COCO we take 5,000 images for validation and testing both parts.
      For BNLIT, we choose 500 images for validation and testing part. For the beginning
      period, we select Flickr8K dataset and we use NVIDIA G1 GAMING GPU for train
      the dataset. Then we choose the Flickr30K and MS COCO dataset. After completing
      training and testing of Flickr8K dataset, then procedure for Flickr30K dataset and
      after completing Flickr30K, we use MS COCO.  We also made an English dataset containing 8000 images.
    </div>

    <div class="about_projevct__container bd-grid">
      <h2>  IMPLEMENTATION AND DESIGN   </h2>
      We need to  resize images  all of our  dataset.  We also  need to  the dataset
  JSON  file,  and  VGG  CNN  features  for our  three  benchmark  dataset Flickr8k,
  Flickr30k, and MSCOCO. We use raw image files of each dataset alongside JSON
  file and  VGG CNN features. The input  is dataset of images and  5  sentence
  descriptions which were collected with Amazon Mechanical Turk. In particular, this
  code base is set up for three benchmark datasets.




  IMAGE PREPROCESSING
  We  introduce  with  a  recurrent  neural  network  model  which  generates
  sentence from the  given  input image.  The model  identifies the  image  region and
  generates natural language description of images. Our approach includes a lowering
  of resolution images that adjusted parts of visual and language modalities through a
  typical  multimodal  embedding.  We  would show  that  how  this  model  can  work  in
  generating texture description  from  decoding images. Moreover,  we obtain  better
  performance compared to benchmark results by earlier attempts.

  This model would surely help not just in recognition of objects but would also serve as an assistant for visually impaired persons with some more applications in real world





  In the training section, all of images are fed as input to RNN and RNN asked
  to predict the word of the sentences. For the prediction part, images are passed to
  RNN and  RNN generates  the sentence  word at  a time  and  we get result  of our
  evaluation with BLEU and METEOR scale.



  MODEL IMPLEMENTATION

  Representing image is most important part for image processing and we get a
  lot of ideas to review many recent works. We watch that sentence description
  make visit  references to objects and their  attributes. The CNN is  pre-prepared on
  ImageNet what's more, finetuned on the 200 classes of the ImageNet Detection
  Challenge. We  maintain the  technique for Girshick et  al. to detect each
  object in each image with  a Region Convolutional Neural Network (RCNN). The
  RCNN model has two parts, a region proposal network and another one  is binary
  mask classifier. Representing  sentence  is  most  important  section  for  our  model.  We established the inter-model relationship and like to represent the words in a sentence in the  same established  h-dimensional space  as the  image regions.

    </div>
    <div class="about_projevct__container bd-grid">
      <h2>HARDWARE REQUIREMENTS </h2>
      <h3><strong>Processor -:</strong>Intel Core i5 and above</h3>
      <h3><strong>RAM-: </strong> 8GB and above</h3>
      <h3><strong>Hard Disk-:</strong> 500 GB and above</h3>
      <h3> <strong>Graphics-: </strong> NVIDIA G1 </h3>
      SOFTWARE REQUIREMENTS

Operating System

Programming Language – Python and JAVA

DATASET

    </div>
    <div class="about_projevct__container bd-grid">
      <h2>     CONCLUSION </h2>
      We  introduce  with a recurrent  neural  network  model  which  generates
  sentence from the  given  input image.  The model  identifies the  image  region and
  generates natural language description of images. Our approach includes a lowering
  of resolution images that adjusted parts of visual and language modalities through a
  typical  multimodal  embedding.  We  would show  that  how  this  model  can  work  in
  generating texture description  from  decoding images. Moreover,  we obtain  better
  performance compared to benchmark results by earlier attempts.

  This model would surely help not just in recognition of objects but would also serve as an assistant for visually impaired persons with some more applications in real world

    </div>

</section>
            <!--===== CONTACT =====-->
            <section class="contact section" id="contact">
                <h2 class="section-title">Contact</h2>

                <div class="contact__container bd-grid">
                    <form action="" class="contact__form">
                        <input type="text" placeholder="Name" class="contact__input">
                        <input type="mail" placeholder="Email" class="contact__input">
                        <textarea name="" id="" cols="0" rows="10" class="contact__input"></textarea>
                        <input type="button" value="Enviar" class="contact__button button">
                    </form>
                </div>
            </section>
        </main>

        <!--===== FOOTER =====-->
        <footer class="footer">
            <p class="footer__title">Amzad Khan</p>
            <div class="footer__social">
                <a href="#" class="footer__icon"><i class='bx bxl-facebook' ></i></a>
                <a href="#" class="footer__icon"><i class='bx bxl-instagram' ></i></a>
                <a href="#" class="footer__icon"><i class='bx bxl-twitter' ></i></a>
            </div>
            <p>&#169; 2020 Mohammad Amzad Khan</p>
        </footer>


        <!--===== SCROLL REVEAL =====-->
        <script src="https://unpkg.com/scrollreveal"></script>

        <!--===== MAIN JS =====-->
        <script src="Assets/script.js"></script>
    </body>
</html>
